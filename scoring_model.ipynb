{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from functools import *\n",
    "from typing import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import pytorch_lightning as pl\n",
    "import sklearn.metrics as metrics\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from transformers import (BertTokenizer,\n",
    "                          BertModel, \n",
    "                          BertForMaskedLM, \n",
    "                          BertForNextSentencePrediction, \n",
    "                          AdamW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = 'bert-large-cased'\n",
    "\n",
    "epochs = 8\n",
    "batch_size = 6\n",
    "max_len = 256\n",
    "\n",
    "# run trainer.fit or not\n",
    "run_fit = True\n",
    "# run trainer.test or not\n",
    "run_test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STSBData(Dataset):\n",
    "    \n",
    "    def __init__(self, file: str):\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(file, sep='\\t', error_bad_lines=False)[['sentence1', 'sentence2', 'score']]\n",
    "        self.data['score'] /= 5\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return tuple(self.data.iloc[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STSBDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 train_file: str,\n",
    "                 dev_file: str,\n",
    "                 test_file: str,\n",
    "                 pretrained_model: str,\n",
    "                 batch_size: int,\n",
    "                 max_len: int,\n",
    "                 train_batch_size: Optional[int] = None,\n",
    "                 val_batch_size: Optional[int] = None,\n",
    "                 test_batch_size: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.train_file = train_file\n",
    "        self.dev_file = dev_file\n",
    "        self.test_file = test_file\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.train_batch_size = train_batch_size or batch_size\n",
    "        self.dev_batch_size = val_batch_size or batch_size\n",
    "        self.test_batch_size = test_batch_size or batch_size\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, batch: List[Tuple[torch.Tensor, ...]]) -> Dict[str, torch.Tensor]:\n",
    "        s1, s2, label = zip(*batch)\n",
    "        pairs = list(zip(s1, s2))\n",
    "        next_sentence_label = torch.tensor(label).long()\n",
    "        encoded = self.tokenizer.batch_encode_plus(batch_text_or_text_pairs=pairs,\n",
    "                                                   max_length=self.max_len,\n",
    "                                                   add_special_tokens=True,\n",
    "                                                   return_token_type_ids=True,\n",
    "                                                   return_attention_mask=True)\n",
    "        input_ids = self._make_batch(encoded['input_ids'])\n",
    "        token_type_ids = self._make_batch(encoded['token_type_ids'])\n",
    "        attention_mask = self._make_batch(encoded['attention_mask'])\n",
    "        return {'input_ids': input_ids, \n",
    "                  'token_type_ids': token_type_ids, \n",
    "                  'attention_mask': attention_mask,\n",
    "                  'next_sentence_label': next_sentence_label}\n",
    "    \n",
    "    def _make_batch(self, sequence: List[List[torch.Tensor]]) -> torch.Tensor:\n",
    "        return rnn.pad_sequence(list(map(torch.tensor, sequence)), batch_first=True)\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.pretrained_model)\n",
    "        self.train_data = STSBData(self.train_file)\n",
    "        self.dev_data = STSBData(self.dev_file)\n",
    "        self.test_data = None\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(dataset=self.train_data,\n",
    "                          batch_size=self.train_batch_size,\n",
    "                          collate_fn=self,\n",
    "                          num_workers=1) # ensure num_workers=1 to avoid duplicated entries in each epoch\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(dataset=self.val_data,\n",
    "                          batch_size=self.val_batch_size,\n",
    "                          collate_fn=self,\n",
    "                          num_workers=1) # ensure num_workers=1 to avoid duplicated entries in each epoch\n",
    "    \n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(dataset=self.test_data,\n",
    "                          batch_size=self.test_batch_size,\n",
    "                          collate_fn=self,\n",
    "                          num_workers=1) # ensure num_workers=1 to avoid duplicated entries in each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 2509: expected 10 fields, saw 11\\nSkipping line 2650: expected 10 fields, saw 11\\nSkipping line 2727: expected 10 fields, saw 11\\nSkipping line 3071: expected 10 fields, saw 11\\nSkipping line 3393: expected 10 fields, saw 11\\n'\n",
      "b'Skipping line 1042: expected 10 fields, saw 11\\nSkipping line 1066: expected 10 fields, saw 11\\nSkipping line 1083: expected 10 fields, saw 11\\nSkipping line 1137: expected 10 fields, saw 11\\nSkipping line 1150: expected 10 fields, saw 11\\n'\n"
     ]
    }
   ],
   "source": [
    "dm = STSBDataModule('train.tsv', 'dev.tsv', 'test.tsv', pretrained_model, batch_size, max_len)\n",
    "dm.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   138,  4261,  1110,  1781,  1228,   119,   102,  1760,  1586,\n",
       "           4261,  1110,  1781,  1228,   119,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,   138,  1299,  1110,  1773,   170,  1415, 10284,   119,   102,\n",
       "            138,  1299,  1110,  1773,   170, 10284,   119,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,   138,  1299,  1110,  9243,   188,  8167, 15018,  1181,  9553,\n",
       "           1113,   170, 13473,   119,   102,   138,  1299,  1110,  9243,   188,\n",
       "           8167, 23372,  1174,  9553,  1113,  1126,  8362,  2528, 27499, 13473,\n",
       "            119,   102],\n",
       "         [  101,  2677,  1441,  1132,  1773, 10924,   119,   102,  1960,  1441,\n",
       "           1132,  1773, 10924,   119,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,   138,  1299,  1110,  1773,  1103, 11742,   119,   102,   138,\n",
       "           1299,  8808,  1110,  1773,  1103, 11742,   119,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [  101,  1789,  1441,  1132,  2935,   119,   102,  1960,  1441,  1132,\n",
       "           2935,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'next_sentence_label': tensor([1, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dm.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoringModel(pl.LightningModule):\n",
    "    def __init__(self, *,\n",
    "                 pretrained_model: str, \n",
    "                 learning_rate: float = 0.001):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.nsp = BertForNextSentencePrediction.from_pretrained(pretrained_model)\n",
    "        self.save_hyperparameters('pretrained_model')\n",
    "    \n",
    "    def forward(self, \n",
    "                input_ids: torch.Tensor,\n",
    "                token_type_ids: torch.Tensor, \n",
    "                attention_mask: torch.Tensor, \n",
    "                next_sentence_label: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        outputs = self.nsp(input_ids=input_ids, \n",
    "                           token_type_ids=token_type_ids, \n",
    "                           attention_mask=attention_mask, \n",
    "                           next_sentence_label=next_sentence_label)\n",
    "        return outputs\n",
    "    \n",
    "    def configure_optimizers(self) -> Optimizer:\n",
    "        self.optimizer = AdamW(model.parameters(), lr=self.learning_rate, weight_decay=0.01)\n",
    "        self.scheduler = ExponentialLR(self.optimizer, gamma=0.9)\n",
    "        return self.optimizer\n",
    "    \n",
    "    def training_step(self, batch: torch.Tensor, batch_idx: int) -> pl.TrainResult:\n",
    "        losses, score = self(**batch)\n",
    "        loss = losses.mean()\n",
    "        result = pl.TrainResult(loss)\n",
    "        result.log_dict({'train_loss': loss,\n",
    "                         'lr': self.scheduler.get_lr()[0]}, \n",
    "                        on_step=True, \n",
    "                        on_epoch=False, \n",
    "                        logger=True, \n",
    "                        prog_bar=True)\n",
    "        return result\n",
    "    \n",
    "    def training_epoch_end(self, results: pl.EvalResult) -> pl.TrainResult:\n",
    "        # Shrink learning rate after each epoch\n",
    "        self.scheduler.step()\n",
    "        return results\n",
    "    \n",
    "    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> pl.EvalResult:\n",
    "        losses, score = self(**batch)\n",
    "        loss = losses.mean()\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "        result.log_dict({'val_loss': loss}, \n",
    "                        on_step=True, \n",
    "                        on_epoch=False, \n",
    "                        logger=True, \n",
    "                        prog_bar=True)\n",
    "        result.label = batch['next_sentence_label'].tolist()\n",
    "        result.pred = score.argmax(dim=1).tolist()\n",
    "        return result\n",
    "    \n",
    "    def validation_epoch_end(self, results: pl.EvalResult) -> pl.EvalResult:\n",
    "        label = sum(results.label, [])\n",
    "        pred = sum(results.pred, [])\n",
    "        accuracy = metrics.accuracy_score(label, pred)\n",
    "        precision = metrics.precision_score(label, pred)\n",
    "        recall = metrics.recall_score(label, pred)\n",
    "        f1 = metrics.f1_score(label, pred)\n",
    "        confusion = metrics.confusion_matrix(label, pred)\n",
    "        scores = f'accuracy: {accuracy:.2f}, precision: {precision:.2f}, recall: {recall:.2f}, f1: {f1:.2f}'\n",
    "        title = 'sanity check' if self.current_epoch == 0 else f'epoch {self.current_epoch}'\n",
    "        \n",
    "        # printing only from master process\n",
    "        self.print(f' {title} '.center(len(scores), \"=\"))\n",
    "        self.print(pd.DataFrame(confusion, \n",
    "                           columns=['pred: -', 'pred: +'], \n",
    "                           index=['label: -', 'label: +']))\n",
    "        self.print(scores)\n",
    "        return results\n",
    "    \n",
    "    def test_step(self, batch: torch.Tensor, batch_idx: int) -> pl.EvalResult:\n",
    "        score, = self(**batch)\n",
    "        result = pl.EvalResult()\n",
    "        result.score = score[:, 0].tolist()\n",
    "        return result\n",
    "    \n",
    "    def test_epoch_end(self, results: pl.EvalResult) -> pl.EvalResult:\n",
    "        torch.save(torch.tensor(sum(results.score, [])), 'predictions.pt')\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CtrlRegMappingNSPDataModule(data_file=data_file, \n",
    "                                      pretrained_model=pretrained_model, \n",
    "                                      batch_size=batch_size,\n",
    "                                      max_len=max_len)\n",
    "model = CtrlRegAlignerModel(pretrained_model=pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=1, \n",
    "                     max_epochs=epochs,\n",
    "                     auto_lr_find=True,\n",
    "                     accumulate_grad_batches=4,\n",
    "                     precision=32) # Due to a bug in pytorch-lightning, 16 bit training won't work with auto_lr_find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============================\n",
      "missing regs for train split:\n",
      "=============================\n",
      "CFR:12:1026:1026.19\n",
      "CFR:12:1026:1026.21\n",
      "CFR:12:1026:1026.23:a\n",
      "CFR:12:1026:1026.23:b\n",
      "CFR:12:1026:1026.24:a\n",
      "CFR:12:1026:1026.24:c\n",
      "CFR:12:1026:1026.24:d\n",
      "CFR:12:1026:1026.24:f:2\n",
      "CFR:12:1026:1026.24:f:3\n",
      "CFR:12:1026:1026.24:g\n",
      "CFR:12:1026:1026.24:h\n",
      "CFR:12:1026:1026.24:i\n",
      "CFR:12:1026:1026.26\n",
      "CFR:12:1026:1026.32:a\n",
      "CFR:12:1026:1026.32:c\n",
      "CFR:12:1026:1026.32:d\n",
      "CFR:12:1026:1026.34\n",
      "CFR:12:1026:1026.35\n",
      "CFR:12:1026:1026.40:a\n",
      "CFR:12:1026:1026.40:b\n",
      "CFR:12:1026:1026.40:c\n",
      "CFR:12:1026:1026.40:d\n",
      "CFR:12:1026:1026.40:e\n",
      "CFR:12:1026:1026.40:f\n",
      "CFR:12:1026:1026.42\n",
      "CFR:12:1026:1026.51:a\n",
      "CFR:12:1026:1026.51:b\n",
      "CFR:12:1026:1026.5:b:i\n",
      "CFR:12:1026:1026.5:b:ii\n",
      "CFR:12:1026:1026.60:a\n",
      "CFR:12:1026:1026.60:b\n",
      "CFR:12:1026:1026.60:c\n",
      "CFR:12:1026:1026.6:a\n",
      "Data quality check on DCO applications \n",
      "USC:15:41:1681g:e\n",
      "USC:15:41:1681g:g:1\n",
      "===========================\n",
      "missing regs for val split:\n",
      "===========================\n",
      "USC:15:41:1681g:e\n",
      "\n",
      "  | Name | Type                          | Params\n",
      "-------------------------------------------------------\n",
      "0 | nsp  | BertForNextSentencePrediction | 333 M \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== sanity check ====================\n",
      "          pred: -  pred: +\n",
      "label: -        4        2\n",
      "label: +        5        1\n",
      "accuracy: 0.42, precision: 0.33, recall: 0.17, f1: 0.22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8163010e264164becf21e7e8668c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Finding best initial lr', style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early due to diverging loss.\n",
      "Learning rate set to 1.9054607179632464e-05\n",
      "\n",
      "  | Name | Type                          | Params\n",
      "-------------------------------------------------------\n",
      "0 | nsp  | BertForNextSentencePrediction | 333 M \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== sanity check ====================\n",
      "          pred: -  pred: +\n",
      "label: -        6        0\n",
      "label: +        6        0\n",
      "accuracy: 0.50, precision: 0.00, recall: 0.00, f1: 0.00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259a712a50344f589d6c6a0c3f13b3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= epoch 1 =======================\n",
      "          pred: -  pred: +\n",
      "label: -       46      196\n",
      "label: +        7      235\n",
      "accuracy: 0.58, precision: 0.55, recall: 0.97, f1: 0.70\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= epoch 2 =======================\n",
      "          pred: -  pred: +\n",
      "label: -       68      174\n",
      "label: +       13      229\n",
      "accuracy: 0.61, precision: 0.57, recall: 0.95, f1: 0.71\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= epoch 3 =======================\n",
      "          pred: -  pred: +\n",
      "label: -       82      160\n",
      "label: +       12      230\n",
      "accuracy: 0.64, precision: 0.59, recall: 0.95, f1: 0.73\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= epoch 4 =======================\n",
      "          pred: -  pred: +\n",
      "label: -      112      130\n",
      "label: +       24      218\n",
      "accuracy: 0.68, precision: 0.63, recall: 0.90, f1: 0.74\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= epoch 5 =======================\n",
      "          pred: -  pred: +\n",
      "label: -       90      152\n",
      "label: +       17      225\n",
      "accuracy: 0.65, precision: 0.60, recall: 0.93, f1: 0.73\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= epoch 6 =======================\n",
      "          pred: -  pred: +\n",
      "label: -       87      155\n",
      "label: +       14      228\n",
      "accuracy: 0.65, precision: 0.60, recall: 0.94, f1: 0.73\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= epoch 7 =======================\n",
      "          pred: -  pred: +\n",
      "label: -       61      181\n",
      "label: +        6      236\n",
      "accuracy: 0.61, precision: 0.57, recall: 0.98, f1: 0.72\n"
     ]
    }
   ],
   "source": [
    "if run_fit:\n",
    "    warnings.filterwarnings('ignore')\n",
    "    trainer.fit(model, datamodule=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "=============================\n",
      "missing regs for train split:\n",
      "=============================\n",
      "CFR:12:1026:1026.19\n",
      "CFR:12:1026:1026.21\n",
      "CFR:12:1026:1026.23:a\n",
      "CFR:12:1026:1026.23:b\n",
      "CFR:12:1026:1026.24:a\n",
      "CFR:12:1026:1026.24:c\n",
      "CFR:12:1026:1026.24:d\n",
      "CFR:12:1026:1026.24:f:2\n",
      "CFR:12:1026:1026.24:f:3\n",
      "CFR:12:1026:1026.24:g\n",
      "CFR:12:1026:1026.24:h\n",
      "CFR:12:1026:1026.24:i\n",
      "CFR:12:1026:1026.26\n",
      "CFR:12:1026:1026.32:a\n",
      "CFR:12:1026:1026.32:c\n",
      "CFR:12:1026:1026.32:d\n",
      "CFR:12:1026:1026.34\n",
      "CFR:12:1026:1026.35\n",
      "CFR:12:1026:1026.40:a\n",
      "CFR:12:1026:1026.40:b\n",
      "CFR:12:1026:1026.40:c\n",
      "CFR:12:1026:1026.40:d\n",
      "CFR:12:1026:1026.40:e\n",
      "CFR:12:1026:1026.40:f\n",
      "CFR:12:1026:1026.42\n",
      "CFR:12:1026:1026.51:a\n",
      "CFR:12:1026:1026.51:b\n",
      "CFR:12:1026:1026.5:b:i\n",
      "CFR:12:1026:1026.5:b:ii\n",
      "CFR:12:1026:1026.60:a\n",
      "CFR:12:1026:1026.60:b\n",
      "CFR:12:1026:1026.60:c\n",
      "CFR:12:1026:1026.6:a\n",
      "Data quality check on DCO applications \n",
      "USC:15:41:1681g:e\n",
      "USC:15:41:1681g:g:1\n",
      "===========================\n",
      "missing regs for val split:\n",
      "===========================\n",
      "USC:15:41:1681g:e\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b7a1b8653948a493570057761ae0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\n",
      "{'CDD Review of Regional Public Files': ['CFR:12:1003:1003.5:e', 'CFR:12:1003:1003.5:b:2', 'CFR:12:25:25.43:b:2', 'CFR:12:25:25.43:a:2', 'CFR:12:25:25.43:b:1:ii'], 'Incentive Plan Document Distribution': ['CFR:12:222:222.90:d:1', 'CFR:12:222:222.90:d:2:iv', 'CFR:12:1022:1022.137:a:2:iii:C', 'CFR:12:222:222.90:d:2:i', 'CFR:12:222:222.90:d:2:iii'], 'Quality Check on Dating of Payment Activity': ['CFR:12:1024:1024.35:e:3:i:C', 'CFR:12:1022:1022.23:a:4', 'CFR:12:1022:1022.42:c', 'CFR:12:1024:1024.35:d', 'CFR:12:1024:1024.35:e:3:i:A'], 'Quality check on manual approvals': ['CFR:12:25:25.24:a', 'CFR:12:25:25.43:b:3:i', 'CFR:12:25:25.43:a:2', 'CFR:12:1003:1003.5:e', 'CFR:12:1026:1026.5:a:3:vi']}\n"
     ]
    }
   ],
   "source": [
    "if run_test:\n",
    "    # I believe this is a bug of pytorch_lightning. If I use auto_lr_find \n",
    "    # for trainer, I can't reuse the trainer and model for testing, and I have \n",
    "    # to use a separate tester and load model from checkpoint like this\n",
    "    tester = pl.Trainer(gpus=1, auto_lr_find=False) \n",
    "    test_model = CtrlRegAlignerModel(pretrained_model=pretrained_model)\n",
    "    tester.test(test_model, ckpt_path=testing_ckpt, datamodule=dataset)\n",
    "    scores = torch.load('predictions.pt')\n",
    "    prediction = dataset.get_top_k_predictions_from_test_results(scores, k=5)\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m51",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m51"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
